import dateparser
from concurrent.futures import ThreadPoolExecutor, as_completed
import csv, json
import psycopg2
from airflow import settings
from psycopg2.extras import execute_values
from psycopg2.extensions import register_adapter, AsIs
from airflow import settings
from sqlalchemy import create_engine
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.bash_operator import BashOperator
from airflow.utils.log.logging_mixin import LoggingMixin
from typing import Callable
from airflow.utils.task_group import TaskGroup
import logging
from logging import handlers
from airflow import models
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.hooks.base_hook import BaseHook
from airflow.models import Variable
import time
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import os
import requests
from bs4 import BeautifulSoup
from airflow.utils.dates import days_ago
import re


# Connections settings
# Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ð¹ Ð¸Ð· JSON Ñ„Ð°Ð¹Ð»Ð°
with open('/opt/airflow/dags/config_connections.json', 'r') as conn_file:
    connections_config = json.load(conn_file)

# ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ð¸ ÑÐ¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ„Ð¸Ð³ Ð´Ð»Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
conn_config = connections_config['psql_connect']

config = {
    'database': conn_config['database'],
    'user': conn_config['user'],
    'password': conn_config['password'],
    'host': conn_config['host'],
    'port': conn_config['port']
}

conn = psycopg2.connect(**config)

raw_tables = ['raw_get_match']

# ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ
default_args = {
    "owner": "admin_1T",
    'start_date': days_ago(1)
}


logging_level = os.environ.get('LOGGING_LEVEL', 'DEBUG').upper()
logging.basicConfig(level=logging_level)
log = logging.getLogger(__name__)
log_handler = handlers.RotatingFileHandler('/opt/airflow/logs/airflow.log',
                                           maxBytes=5000,
                                           backupCount=5)

log_handler.setLevel(logging.DEBUG)
log_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
log_handler.setFormatter(log_formatter)
log.addHandler(log_handler)

class DatabaseManager:
    def __init__(self, conn):
        self.conn = conn
        self.cur = conn.cursor()
        self.raw_tables = raw_tables
        self.log = LoggingMixin().log

    def create_raw_tables(self):     
        table_name = 'raw_get_match'
        try:
            drop_table_query = f"DROP TABLE IF EXISTS {table_name};"
            self.cur.execute(drop_table_query)
            self.log.info(f'Ð£Ð´Ð°Ð»ÐµÐ½Ð° Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° {table_name}')

            create_table_query = f"""                
            CREATE TABLE {table_name}(
               vacancy_id VARCHAR(2083),
               vacancy_name VARCHAR(255),
               towns VARCHAR(255),
               level VARCHAR(255),
               company VARCHAR(255),
               salary_from BIGINT,
               salary_to BIGINT,
               exp_from SMALLINT,
               exp_to SMALLINT,
               description TEXT,
               job_type VARCHAR(255),
               job_format VARCHAR(255),
               languages VARCHAR(255),
               skills VARCHAR(511),
               source_vac VARCHAR(255),
               date_created DATE,
               date_of_download DATE, 
               status VARCHAR(32),
               date_closed DATE,
               version_vac INTEGER,
               actual SMALLINT,
               PRIMARY KEY(vacancy_id)
            );
            """
            self.cur.execute(create_table_query)
            self.log.info(f'Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð° {table_name} ÑÐ¾Ð·Ð´Ð°Ð½Ð° Ð² Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ….')
            self.conn.commit()
        except Exception as e:
            self.conn.rollback()
            self.log.error(f'ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ {table_name}: {e}')



class BaseJobParser:
    def __init__(self, conn, log):
        self.conn = conn
        self.log = log

    def find_vacancies(self, conn):
        """
        ÐœÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹, Ð´Ð¾Ð»Ð¶ÐµÐ½ Ð±Ñ‹Ñ‚ÑŒ Ð¿ÐµÑ€ÐµÐ¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½ Ð² Ð½Ð°ÑÐ»ÐµÐ´Ð½Ð¸ÐºÐ°Ñ…
        """
        raise NotImplementedError("Ð’Ñ‹ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ Ð¼ÐµÑ‚Ð¾Ð´ find_vacancies")

    def save_df(self):
        """
        ÐœÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¸Ð· pandas DataFrame
        """
        raise NotImplementedError("Ð’Ñ‹ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸Ñ‚ÑŒ Ð¼ÐµÑ‚Ð¾Ð´ save_df")
   


class GetMatchJobParser(BaseJobParser):

    def __init__(self, conn, log):
        super().__init__(conn, log)
        self.items = []
        
    def find_vacancies(self):
        BASE_URL = 'https://getmatch.ru/vacancies?p=1&sa=150000&pa=all&s=landing_ca_header'
        url = 'https://getmatch.ru/vacancies?p={i}&sa=150000&pa=all&s=landing_ca_header'
        HEADERS = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:98.0) Gecko/20100101 Firefox/98.0",
}
        self.log.info(f'Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿ÑƒÑÑ‚Ð¾Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº')
        self.items = []
        self.unique_items = []
        seen_ids = set()
        self.all_links = []  
        self.log.info(f'ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ')
        # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð»Ð¸Ð½ÐºÐ¸ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹ Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñ‹ (Ð½Ð° ÑÐ°Ð¹Ñ‚Ðµ Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð½Ð¾ 50ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†, 100 - ÑÑ‚Ð¾ Ñ Ð¸Ð·Ð±Ñ‹Ñ‚ÐºÐ¾Ð¼)     
        
        for i in range(1, 100):
            response = requests.get(url.format(i=i), headers=HEADERS)
            soup = BeautifulSoup(response.content, 'html.parser')
            divs = soup.find_all('div', class_='b-vacancy-card-title')
            for div in divs:
                vacancy_url = 'https://getmatch.ru/' + div.find('a').get('href')
                self.all_links.append(vacancy_url)
    
        for link in self.all_links:                
            resp = requests.get(link, HEADERS)
            vac = BeautifulSoup(resp.content, 'lxml')

            try:
                # Ð¿Ð°Ñ€ÑÐ¸Ð¼ Ð³Ñ€ÐµÐ¹Ð´ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¸ 
                term_element = vac.find('div', {'class': 'col b-term'}, text='Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ')                        
                level = term_element.find_next('div', {'class': 'col b-value'}).text.strip() if term_element else None

                # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð¸Ð½Ð¾ÑÑ‚Ñ€Ð°Ð½Ð½Ñ‹Ðµ ÑÐ·Ñ‹ÐºÐ¸
                lang = vac.find('div', {'class': 'col b-term'}, text='ÐÐ½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ð¹')                        
                if lang is not None:                           
                    level_lang= lang.find_next('span', {'class': 'b-language-description d-md-none'}).text.strip()
                    lang=lang.text.strip()
                    language = f"{lang}: {level_lang}"
                    if level==level_lang:
                        level=None
                else:
                    language=None
                            
                # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð½Ð°Ð²Ñ‹ÐºÐ¸
                stack_container = vac.find('div', class_='b-vacancy-stack-container')
                if stack_container is not None:
                    labels = stack_container.find_all('span', class_='g-label')
                    page_stacks = ', '.join([label.text.strip('][') for label in labels])
                else:
                    page_stacks=None
                    
                # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð²Ð°ÐºÐ°Ð½ÑÐ¸Ð¹
                description_element = vac.find('section', class_='b-vacancy-description')
                description_lines = description_element.stripped_strings
                description = '\n'.join(description_lines)
                
                # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ð¸ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑˆÐ¸Ð²Ð°ÐµÐ¼ Ð·Ð°Ñ€Ð¿Ð»Ð°Ñ‚Ñ‹
                salary_text = vac.find('h3').text.strip()
                salary_text = salary_text.replace('\u200d', '-').replace('â€”', '-')
                if 'â‚½/Ð¼ÐµÑ Ð½Ð° Ñ€ÑƒÐºÐ¸' in vac.find('h3').text:
                    salary_parts = list(map(str.strip, salary_text.split('-')))
                    salary_from = salary_parts[0]
                    if len(salary_parts) == 1:
                        salary_to = None if 'Ð¾Ñ‚' in vac.find('h3').text else salary_parts[0]
                    elif len(salary_parts) > 1:
                        salary_to = salary_parts[2]
                else:
                    salary_from = None
                    salary_to = None
                    
                # ÐŸÑ€Ð¸Ð²Ð¾Ð´Ð¸Ð¼ Ð·Ð°Ñ€Ð¿Ð»Ð°Ñ‚Ñ‹ Ðº Ñ‡Ð¸ÑÐ»Ð¾Ð²Ð¾Ð¼Ñƒ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñƒ 
                if salary_from is not None:
                    numbers = re.findall(r'\d+', salary_from)
                    combined_number = ''.join(numbers)
                    salary_from = int(combined_number) if combined_number else None
                if salary_to is not None:
                    numbers = re.findall(r'\d+', salary_to)
                    combined_number = ''.join(numbers)
                    salary_to = int(combined_number) if combined_number else None 
                           
                # ÐŸÐ°Ñ€ÑÐ¸Ð¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹
                job_form_classes = ['g-label-linen', 'g-label-zanah', 'ng-star-inserted']
                job_form = vac.find('span', class_=job_form_classes)
                job_format = job_form.get_text(strip=True) if job_form is not None else None
                
                # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ñ€ÑƒÐ³Ð¸Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ 
                date_created = date_of_download = datetime.now().date()
                status ='existing'
                version_vac=1
                actual=1
                                         
                item = {
                    "company": vac.find('h2').text.replace('\xa0', ' ').strip('Ð²'),
                    "vacancy_name": vac.find('h1').text,
                    "skills": page_stacks,
                    "towns": ', '.join([span.get_text(strip=True).replace('ðŸ“', '') for span in vac.find_all('span', class_='g-label-secondary')]),
                    "vacancy_id": link,
                    "description": description,
                    "job_format": job_format,
                    "level": level,
                    "salary_from": salary_from,
                    "salary_to": salary_to,
                    "date_created": date_created,
                    "date_of_download": date_of_download,
                    "source_vac": BASE_URL,
                    "status": status,
                    "version_vac": version_vac,
                    "actual": actual,
                    "languages":language,
        }

                print(f"Adding item: {item}")
                self.items.append(item)
                time.sleep(3)

            except AttributeError as e:
                        print(f"Error processing link {link}: {e}")
        # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹
        for item in self.items:
            vacancy_id = item["vacancy_id"]
            if vacancy_id not in seen_ids:
                seen_ids.add(vacancy_id)
                self.unique_items.append(item)

        self.log.info("Ð’ ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ðµ")
        return self.unique_items


    def save_df(self, cursor, connection):
        self.log.info(f"Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð²ÑÑ‚Ð°Ð²ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…")
        try:
            for item in self.unique_items:
                company = item["company"]
                vacancy_title = item["vacancy_name"]
                skills = item["skills"]
                meta = item["towns"]
                link_vacancy = item["vacancy_id"]
                date_created=item["date_created"]
                date_of_download=item["date_of_download"]
                description=item["description"]
                source_vac=item["source_vac"]
                status=item["status"]
                version_vac=item["version_vac"]
                actual=item["actual"]
                level=item["level"]
                salary_from=item["salary_from"]
                salary_to=item["salary_to"]
                job_format=item["job_format"]
                language= item["languages"]

            # SQL-Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð´Ð»Ñ Ð²ÑÑ‚Ð°Ð²ÐºÐ¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…
                sql = """INSERT INTO public.raw_get_match (vacancy_id, company, vacancy_name, skills, towns, description, date_created,
                                                 date_of_download, source_vac, status, version_vac, actual, level, salary_from, salary_to, job_format, languages)
                   VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);"""
                values = (link_vacancy, company, vacancy_title, skills, meta, description, date_created, 
                      date_of_download, source_vac, status, version_vac, actual, level, salary_from, salary_to, job_format, language)
                cursor.execute(sql, values)

            connection.commit()
            cursor.close()
            connection.close()
            print("Data successfully inserted into the database.")
            self.log.info(f"Ð”Ð°Ð½Ð½Ñ‹Ðµ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ñ‹")
            print(f"Inserted data: {self.items}")
        
            self.items = []

        except Exception as e:
            print(f"Error during data insertion: {e}")
            self.log.info(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ñ Ð´Ð°Ð½Ð½Ñ‹Ñ…")
            connection.rollback()
            cursor.close()
            connection.close()

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¾Ð±ÑŠÐµÐºÑ‚ DatabaseManager
db_manager = DatabaseManager(conn=conn)

# Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¾Ð±ÑŠÐµÐºÑ‚ GetMatchJobParser
get_match_parser = GetMatchJobParser(conn=conn, log=log)

def init_run_get_match_parser(**context):
    log.info('Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ð°Ñ€ÑÐµÑ€Ð° Get Match')
    parser = GetMatchJobParser(conn, log)
    items = parser.find_vacancies()
    log.info('ÐŸÐ°Ñ€ÑÐµÑ€ Get Match ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¿Ñ€Ð¾Ð²ÐµÐ» Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ')
    parser.save_df(cursor=conn.cursor(), connection=conn)


with DAG(dag_id = "parse_get_match_jobs", schedule_interval = None, tags=['admin_1T'],
    default_args = default_args, catchup = False) as dag:


# ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸
        create_raw_tables = PythonOperator(
            task_id='create_raw_tables',
            python_callable=db_manager.create_raw_tables,
            provide_context=True
)

        parse_get_match_jobs = PythonOperator(
            task_id='parse_get_match_jobs',
            python_callable=init_run_get_match_parser,
             provide_context=True)

create_raw_tables >> parse_get_match_jobs